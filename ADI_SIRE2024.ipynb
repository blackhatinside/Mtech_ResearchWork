{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e3d03f8-d720-459e-b6bf-81f82006e946",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-29 11:57:17.881872: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-29 11:57:18.035217: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-29 11:57:18.811330: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "from focal_loss import BinaryFocalLoss\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage.io import imshow\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4367b061-7c92-4f2f-9313-c34062b51f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of Folders Inside Training:  250\n",
      "No of Folders Inside Ground Truth:  251\n"
     ]
    }
   ],
   "source": [
    "if os.name == 'nt':\n",
    "    base_path = \"C:\\\\Cyberkid\\\\MyMTech\\\\Labwork\\\\SecondYear\\\\MyWork\\\\Datasets\\\\ISLES-2022\\\\ISLES-2022\"\n",
    "else:\n",
    "    base_path = \"/home/user/Tf_script/dataset/ISLES_2022/\"\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "IMG_SIZE=112\n",
    "PATH_DATASET = base_path\n",
    "PATH_RAWDATA = os.path.join(base_path, \"rawdata\")\n",
    "PATH_DERIVATIVES = os.path.join(base_path, \"derivatives\")\n",
    "OUTPUT_DIRECTORY = \"./output/ISLESfolder\"\n",
    "os.makedirs(OUTPUT_DIRECTORY, exist_ok=True)\n",
    "\n",
    "print(\"No of Folders Inside Training: \", len(os.listdir(PATH_RAWDATA)))\n",
    "print(\"No of Folders Inside Ground Truth: \", len(os.listdir(PATH_DERIVATIVES)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ea20ab6-5a5a-4a21-bdee-b3c9d760ed63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # Functions\n",
    "\n",
    "def get_ids(path):\n",
    "    directories = [f.path for f in os.scandir(path) if f.is_dir()]\n",
    "    ids = []\n",
    "    id_startindex = directories[0].find(\"sub\")\n",
    "    for i in range(len(directories)):\n",
    "        ids.append(directories[i][id_startindex:])\n",
    "    return sorted(ids)\n",
    "\n",
    "def dice_coeff(y_true,y_pred):\n",
    "    y_true_new = K.flatten(y_true)\n",
    "    y_pred_new = K.flatten(y_pred)\n",
    "    denominator = K.sum(y_true_new) + K.sum(y_pred_new)\n",
    "    numerator = K.sum(y_true_new * y_pred_new)\n",
    "    return (2*numerator + 1)/(denominator+1)\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def iou(y_true,y_pred):\n",
    "    intersec = K.sum(y_true * y_pred)\n",
    "    union = K.sum(y_true + y_pred)\n",
    "    iou = (intersec + 0.1) / (union- intersec + 0.1)\n",
    "    return iou\n",
    "\n",
    "def dice_score(y_true, y_pred, smooth = 1e-5):\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred)\n",
    "    dice = (2.0 * intersection + smooth) / (union + smooth)\n",
    "    return dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cb62dc9-0ba6-41ea-bf64-51e0f422aa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # Loss Functions\n",
    "\n",
    "def single_dice_loss(y_true, y_pred):\n",
    "    return 1.0 - dice_score(y_true, y_pred)\n",
    "\n",
    "def binary_crossentropy_loss(y_true, y_pred):\n",
    "    return tf.keras.losses.BinaryCrossentropy()(y_true, y_pred)\n",
    "\n",
    "def binary_focal_loss(gamma=2., alpha=0.25):\n",
    "    def focal_loss(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        alpha_t = y_true * alpha + (K.ones_like(y_true) - y_true) * (1 - alpha)\n",
    "        p_t = y_true * y_pred + (K.ones_like(y_true) - y_true) * (1 - y_pred)\n",
    "        focal_loss = - alpha_t * K.pow((K.ones_like(y_true) - p_t), gamma) * K.log(p_t)\n",
    "        return K.mean(focal_loss)\n",
    "    return focal_loss\n",
    "\n",
    "def dice_crossentropy_loss(y_true, y_pred, alpha=0.5):\n",
    "    dice_loss = 1.0 - dice_score(y_true, y_pred)\n",
    "    crossentropy_loss = tf.keras.losses.BinaryCrossentropy()(y_true, y_pred)\n",
    "    return alpha * dice_loss + (1 - alpha) * crossentropy_loss\n",
    "\n",
    "def dice_focal_loss(y_true, y_pred, gamma=2., alpha=0.25, alpha_dice=0.5):\n",
    "    dice_loss = 1.0 - dice_score(y_true, y_pred)\n",
    "    focal_loss = binary_focal_loss(gamma, alpha)(y_true, y_pred)\n",
    "    return alpha_dice * dice_loss + (1 - alpha_dice) * focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70863db0-d509-4ce4-bcff-d1ab2927f6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # Layers/Blocks\n",
    "\n",
    "def conv_block(inp,filters):\n",
    "    x=Conv2D(filters,(3,3),padding='same',activation='relu')(inp)\n",
    "    x=Conv2D(filters,(3,3),padding='same')(x)\n",
    "    x=BatchNormalization(axis=3)(x)\n",
    "    x=Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def encoder_block(inp,filters):\n",
    "    x=conv_block(inp,filters)\n",
    "    p=MaxPooling2D(pool_size=(2,2))(x)\n",
    "    return x,p\n",
    "\n",
    "def attention_block(l_layer,h_layer):\n",
    "    phi=Conv2D(h_layer.shape[-1],(1,1),padding='same')(l_layer)\n",
    "    theta=Conv2D(h_layer.shape[-1],(1,1),strides=(2,2),padding='same')(h_layer)\n",
    "    x=tf.keras.layers.add([phi,theta])\n",
    "    x=Activation('relu')(x)\n",
    "    x=Conv2D(1,(1,1),padding='same',activation='sigmoid')(x)\n",
    "    x=UpSampling2D(size=(2,2))(x)\n",
    "    x=tf.keras.layers.multiply([h_layer,x])\n",
    "    x=BatchNormalization(axis=3)(x)\n",
    "    return x\n",
    "\n",
    "def decoder_block(inp,filters,concat_layer):\n",
    "    x=Conv2DTranspose(filters,(2,2),strides=(2,2),padding='same')(inp)\n",
    "    x=concatenate([x,concat_layer])\n",
    "    x=conv_block(x,filters)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a8ca8de-cf58-4065-b0b7-9436dd8526f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # Classes\n",
    "\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, list_IDs, dim=(IMG_SIZE,IMG_SIZE), batch_size = 1, n_channels = 1, shuffle=False):\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        Batch_ids = [self.list_IDs[k] for k in indexes]\n",
    "        X, y = self.__data_generation(Batch_ids)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, Batch_ids):\n",
    "        for c, i in enumerate(Batch_ids):\n",
    "            case_path = os.path.join(PATH_RAWDATA, i)\n",
    "            data_path = os.path.join(case_path, 'ses-0001', 'dwi', f'{i}_ses-0001_dwi.nii.gz');\n",
    "            dwi = nib.load(data_path).get_fdata()\n",
    "            dwi=scaler.fit_transform(dwi.reshape(-1, dwi.shape[-1])).reshape(dwi.shape)\n",
    "            slices = dwi.shape[2]\n",
    "            X = np.zeros((slices, 112,112, 1))\n",
    "            case_path2 = os.path.join(PATH_DERIVATIVES, i)\n",
    "            data_path_2 = os.path.join(case_path2, 'ses-0001', f'{i}_ses-0001_msk.nii.gz');\n",
    "            msk = nib.load(data_path_2).get_fdata()\n",
    "            msk_slices = msk.shape[2]\n",
    "            y = np.zeros((msk_slices, 112,112))\n",
    "            for j in range(slices):\n",
    "                X[j,:,:,0] = cv2.resize(dwi[:,:,j+0], (IMG_SIZE, IMG_SIZE));\n",
    "                X=X.astype(np.float32)\n",
    "                y[j] = cv2.resize(msk[:,:,j+0],(112,112));\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6663e15-4a91-4826-ab84-df03d5fd7f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of train_ids: 250\n",
      "No of mask_ids: 250\n",
      "\n",
      "train, validate, test:  [191, 25, 34]\n",
      "train, validate, test:  [191, 25, 34]\n"
     ]
    }
   ],
   "source": [
    "train_ids = get_ids(PATH_RAWDATA)\n",
    "mask_ids = get_ids(PATH_DERIVATIVES)\n",
    "\n",
    "print(\"No of train_ids: {}\\nNo of mask_ids: {}\\n\".format(len(train_ids), len(mask_ids)))\n",
    "\n",
    "train_test_ids, val_ids,train_test_mask, val_mask = train_test_split(train_ids,mask_ids,test_size=0.1)\n",
    "train_ids,  test_ids, train_mask , test_mask = train_test_split(train_test_ids,train_test_mask,test_size=0.15)\n",
    "\n",
    "tvt_ids = [train_ids, val_ids, test_ids]\n",
    "print(\"train, validate, test: \", list(map(len, tvt_ids)))\n",
    "\n",
    "training_generator = DataGenerator(train_ids)\n",
    "val_generator = DataGenerator(val_ids)\n",
    "test_generator = DataGenerator(test_ids)\n",
    "\n",
    "tvt_generator = [training_generator, val_generator, test_generator]\n",
    "print(\"train, validate, test: \", list(map(len, tvt_generator)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba598bde-244a-4b42-badd-728b5f505a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-29 11:57:20.698614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14400 MB memory:  -> device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:19:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "VAL_EPOCH = 30\n",
    "VAL_PATIENCE = 40\n",
    "\n",
    "inputs=Input((112,112,1))\n",
    "d1,p1=encoder_block(inputs,64)\n",
    "d2,p2=encoder_block(p1,128)\n",
    "d3,p3=encoder_block(p2,256)\n",
    "d4,p4=encoder_block(p3,512)\n",
    "b1=conv_block(p4,1024)\n",
    "e2=decoder_block(b1,512,d4)\n",
    "e3=decoder_block(e2,256,d3)\n",
    "e4=decoder_block(e3,128,d2)\n",
    "e5=decoder_block(e4,64,d1)\n",
    "outputs = Conv2D(1, (1,1),activation=\"sigmoid\")(e5)\n",
    "\n",
    "model=Model(inputs=[inputs], outputs=[outputs],name='AttentionUnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b90fc172-4ff5-4c23-a3b8-c865310f762a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"AttentionUnet\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 112, 112, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 112, 112, 64  640         ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 112, 112, 64  36928       ['conv2d[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 112, 112, 64  256        ['conv2d_1[0][0]']               \n",
      " alization)                     )                                                                 \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 112, 112, 64  0           ['batch_normalization[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 56, 56, 64)   0           ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 56, 56, 128)  73856       ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 56, 56, 128)  147584      ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 56, 56, 128)  512        ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 56, 56, 128)  0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 28, 28, 128)  0          ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 28, 28, 256)  295168      ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 28, 28, 256)  590080      ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 28, 28, 256)  1024       ['conv2d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 28, 28, 256)  0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 14, 14, 256)  0          ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 14, 14, 512)  1180160     ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 14, 14, 512)  2359808     ['conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 14, 14, 512)  2048       ['conv2d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 14, 14, 512)  0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 7, 7, 512)   0           ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 7, 7, 1024)   4719616     ['max_pooling2d_3[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 7, 7, 1024)   9438208     ['conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 7, 7, 1024)  4096        ['conv2d_9[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 7, 7, 1024)   0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTransp  (None, 14, 14, 512)  2097664    ['activation_4[0][0]']           \n",
      " ose)                                                                                             \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 14, 14, 1024  0           ['conv2d_transpose[0][0]',       \n",
      "                                )                                 'activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 14, 14, 512)  4719104     ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 14, 14, 512)  2359808     ['conv2d_10[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 14, 14, 512)  2048       ['conv2d_11[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 14, 14, 512)  0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2DTran  (None, 28, 28, 256)  524544     ['activation_5[0][0]']           \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 28, 28, 512)  0           ['conv2d_transpose_1[0][0]',     \n",
      "                                                                  'activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 28, 28, 256)  1179904     ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 28, 28, 256)  590080      ['conv2d_12[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 28, 28, 256)  1024       ['conv2d_13[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 28, 28, 256)  0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2DTran  (None, 56, 56, 128)  131200     ['activation_6[0][0]']           \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 56, 56, 256)  0           ['conv2d_transpose_2[0][0]',     \n",
      "                                                                  'activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 56, 56, 128)  295040      ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 56, 56, 128)  147584      ['conv2d_14[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 56, 56, 128)  512        ['conv2d_15[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 56, 56, 128)  0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_transpose_3 (Conv2DTran  (None, 112, 112, 64  32832      ['activation_7[0][0]']           \n",
      " spose)                         )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 112, 112, 12  0           ['conv2d_transpose_3[0][0]',     \n",
      "                                8)                                'activation[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 112, 112, 64  73792       ['concatenate_3[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 112, 112, 64  36928       ['conv2d_16[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 112, 112, 64  256        ['conv2d_17[0][0]']              \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 112, 112, 64  0           ['batch_normalization_8[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 112, 112, 1)  65          ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 31,042,369\n",
      "Trainable params: 31,036,481\n",
      "Non-trainable params: 5,888\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=single_dice_loss,\n",
    "    # loss = binary_crossentropy_loss,\n",
    "    # loss = binary_focal_loss(gamma=2.0, alpha=0.25),\n",
    "    # loss = dice_crossentropy_loss,\n",
    "    # loss = dice_focal_loss,\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    metrics = ['accuracy', dice_coeff,dice_score,iou, precision]\n",
    ")\n",
    "\n",
    "# # # {\n",
    "\n",
    "# Add batch normalization and dropout to the encoder and decoder blocks\n",
    "encoder_block = lambda x, filters: Conv2D(filters, (3, 3), padding='same')(x)\n",
    "encoder_block = lambda x, filters: BatchNormalization()(encoder_block(x, filters))\n",
    "encoder_block = lambda x, filters: Dropout(0.2)(encoder_block(x, filters))\n",
    "\n",
    "decoder_block = lambda x, filters: Conv2DTranspose(filters, (2, 2), strides=(2, 2), padding='same')(x)\n",
    "decoder_block = lambda x, filters: BatchNormalization()(decoder_block(x, filters))\n",
    "decoder_block = lambda x, filters: Dropout(0.2)(decoder_block(x, filters))\n",
    "\n",
    "# Add batch normalization and dropout to the attention block\n",
    "attention_block = lambda x, h_layer: Conv2D(h_layer.shape[-1], (1, 1), padding='same')(x)\n",
    "attention_block = lambda x, h_layer: BatchNormalization()(attention_block(x, h_layer))\n",
    "attention_block = lambda x, h_layer: Dropout(0.2)(attention_block(x, h_layer))\n",
    "\n",
    "# Compile the model with the new layers\n",
    "model.compile(\n",
    "    loss=single_dice_loss,\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    metrics=['accuracy', dice_coeff, dice_score, iou, precision]\n",
    ")\n",
    "\n",
    "# # # }\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e273a3-2034-46a8-a977-f18ea4d7d2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-29 11:57:21.642682: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2024-10-29 11:57:25.279762: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2024-10-29 11:57:28.575167: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7fdd1ce8c5c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-10-29 11:57:28.575195: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA RTX A4000, Compute Capability 8.6\n",
      "2024-10-29 11:57:28.592306: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-10-29 11:57:28.805981: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191/191 [==============================] - ETA: 0s - loss: 0.9402 - accuracy: 0.9268 - dice_coeff: 0.0613 - dice_score: 0.0613 - iou: 0.0342 - precision: 0.0754"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-29 11:59:56.664520: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.99775, saving model to DiceLoss_ISLES22_2DAttention_wts.h5\n",
      "191/191 [==============================] - 164s 744ms/step - loss: 0.9402 - accuracy: 0.9268 - dice_coeff: 0.0613 - dice_score: 0.0613 - iou: 0.0342 - precision: 0.0754 - val_loss: 0.9997 - val_accuracy: 0.9977 - val_dice_coeff: 4.4023e-04 - val_dice_score: 4.0077e-04 - val_iou: 2.0444e-04 - val_precision: 0.0000e+00\n",
      "Epoch 2/30\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.7523 - accuracy: 0.9941 - dice_coeff: 0.2660 - dice_score: 0.2657 - iou: 0.1746 - precision: 0.3528\n",
      "Epoch 2: val_accuracy improved from 0.99775 to 0.99779, saving model to DiceLoss_ISLES22_2DAttention_wts.h5\n",
      "191/191 [==============================] - 55s 287ms/step - loss: 0.7523 - accuracy: 0.9941 - dice_coeff: 0.2660 - dice_score: 0.2657 - iou: 0.1746 - precision: 0.3528 - val_loss: 0.9726 - val_accuracy: 0.9978 - val_dice_coeff: 0.0283 - val_dice_score: 0.0276 - val_iou: 0.0145 - val_precision: 0.5646\n",
      "Epoch 3/30\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.9088 - accuracy: 0.9789 - dice_coeff: 0.0965 - dice_score: 0.0960 - iou: 0.0584 - precision: 0.1109\n",
      "Epoch 3: val_accuracy did not improve from 0.99779\n",
      "191/191 [==============================] - 55s 285ms/step - loss: 0.9088 - accuracy: 0.9789 - dice_coeff: 0.0965 - dice_score: 0.0960 - iou: 0.0584 - precision: 0.1109 - val_loss: 0.9830 - val_accuracy: 0.9977 - val_dice_coeff: 0.0184 - val_dice_score: 0.0135 - val_iou: 0.0082 - val_precision: 0.0554\n",
      "Epoch 4/30\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.6645 - accuracy: 0.9966 - dice_coeff: 0.3634 - dice_score: 0.3623 - iou: 0.2519 - precision: 0.4763\n",
      "Epoch 4: val_accuracy did not improve from 0.99779\n",
      "191/191 [==============================] - 55s 285ms/step - loss: 0.6645 - accuracy: 0.9966 - dice_coeff: 0.3634 - dice_score: 0.3623 - iou: 0.2519 - precision: 0.4763 - val_loss: 0.7786 - val_accuracy: 0.9961 - val_dice_coeff: 0.2508 - val_dice_score: 0.2503 - val_iou: 0.1624 - val_precision: 0.3388\n",
      "Epoch 5/30\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.6018 - accuracy: 0.9972 - dice_coeff: 0.4292 - dice_score: 0.4277 - iou: 0.3104 - precision: 0.5307\n",
      "Epoch 5: val_accuracy improved from 0.99779 to 0.99844, saving model to DiceLoss_ISLES22_2DAttention_wts.h5\n",
      "191/191 [==============================] - 55s 287ms/step - loss: 0.6018 - accuracy: 0.9972 - dice_coeff: 0.4292 - dice_score: 0.4277 - iou: 0.3104 - precision: 0.5307 - val_loss: 0.6083 - val_accuracy: 0.9984 - val_dice_coeff: 0.4277 - val_dice_score: 0.4267 - val_iou: 0.2989 - val_precision: 0.5170\n",
      "Epoch 6/30\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.5803 - accuracy: 0.9973 - dice_coeff: 0.4485 - dice_score: 0.4457 - iou: 0.3250 - precision: 0.5579\n",
      "Epoch 6: val_accuracy did not improve from 0.99844\n",
      "191/191 [==============================] - 54s 283ms/step - loss: 0.5803 - accuracy: 0.9973 - dice_coeff: 0.4485 - dice_score: 0.4457 - iou: 0.3250 - precision: 0.5579 - val_loss: 0.9096 - val_accuracy: 0.9657 - val_dice_coeff: 0.1122 - val_dice_score: 0.1122 - val_iou: 0.0680 - val_precision: 0.0714\n",
      "Epoch 7/30\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.5690 - accuracy: 0.9973 - dice_coeff: 0.4617 - dice_score: 0.4560 - iou: 0.3356 - precision: 0.5699\n",
      "Epoch 7: val_accuracy did not improve from 0.99844\n",
      "191/191 [==============================] - 54s 284ms/step - loss: 0.5690 - accuracy: 0.9973 - dice_coeff: 0.4617 - dice_score: 0.4560 - iou: 0.3356 - precision: 0.5699 - val_loss: 0.5600 - val_accuracy: 0.9984 - val_dice_coeff: 0.4746 - val_dice_score: 0.4734 - val_iou: 0.3407 - val_precision: 0.5048\n",
      "Epoch 8/30\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.5440 - accuracy: 0.9974 - dice_coeff: 0.4842 - dice_score: 0.4815 - iou: 0.3596 - precision: 0.5896\n",
      "Epoch 8: val_accuracy did not improve from 0.99844\n",
      "191/191 [==============================] - 54s 283ms/step - loss: 0.5440 - accuracy: 0.9974 - dice_coeff: 0.4842 - dice_score: 0.4815 - iou: 0.3596 - precision: 0.5896 - val_loss: 0.7113 - val_accuracy: 0.9980 - val_dice_coeff: 0.2907 - val_dice_score: 0.2873 - val_iou: 0.1910 - val_precision: 0.7130\n",
      "Epoch 9/30\n",
      "103/191 [===============>..............] - ETA: 24s - loss: 0.5324 - accuracy: 0.9968 - dice_coeff: 0.4999 - dice_score: 0.4989 - iou: 0.3720 - precision: 0.6067"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(\n",
    "    'DiceLoss_ISLES22_2DAttention_wts.h5',\n",
    "    monitor='val_accuracy',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=VAL_PATIENCE,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "att_unet_history = model.fit(\n",
    "    training_generator,\n",
    "    steps_per_epoch=len(train_ids),\n",
    "    validation_data=val_generator,\n",
    "    callbacks= [checkpoint,early_stop],\n",
    "    epochs=VAL_EPOCH\n",
    ")\n",
    "\n",
    "test_wt=model.predict(test_generator)\n",
    "print(\"test_wt.shape: \", test_wt.shape)\n",
    "\n",
    "results = model.evaluate(test_generator, steps=len(test_ids))\n",
    "print(\"RESULTS: \", results)\n",
    "# print(\"Test loss: \",results[0])\n",
    "# print(\"Test Dice: \",results[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921d33a9-eed7-4170-9053-ea26c51f0f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(3,3))\n",
    "ax.imshow(test_wt[10,:,:,:],cmap='gray')\n",
    "\n",
    "y_pred_thresholded = test_wt > 0.4\n",
    "fig, ax = plt.subplots(1,1, figsize=(3,3))\n",
    "ax.imshow(y_pred_thresholded[10,:,:,:],cmap='gray')\n",
    "\n",
    "def dice_coeff(y_true,y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    y_true_new = K.flatten(y_true)\n",
    "    y_pred_new = K.flatten(y_pred)\n",
    "    denominator = K.sum(y_true_new) + K.sum(y_pred_new)\n",
    "    if denominator == 0.0:\n",
    "        return 1.0\n",
    "    numerator = K.sum(y_true_new * y_pred_new)\n",
    "    return (2.0*numerator)/(denominator)\n",
    "\n",
    "def iou(y_true,y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    y_true = K.flatten(y_true)\n",
    "    y_pred = K.flatten(y_pred)\n",
    "    intersec = K.sum(y_true * y_pred)\n",
    "    union = K.sum(y_true + y_pred)\n",
    "    if union == 0.0:\n",
    "        return 1.0\n",
    "    iou = (intersec) / (union- intersec)\n",
    "    return iou\n",
    "\n",
    "loss_values = []\n",
    "dice_values = []\n",
    "iou_values = []\n",
    "\n",
    "for batch_x, batch_y in test_generator:\n",
    "    mask_image = np.expand_dims(batch_y, axis=-1)\n",
    "    y_predwts = model.predict(batch_x)\n",
    "    y_pred = np.where(y_predwts < 0.2, 0.0, y_predwts).astype(np.float32)\n",
    "    y_pred_thresholded = y_pred\n",
    "    for i in range(len(batch_x)):\n",
    "        dice = dice_coeff(batch_y[i], y_pred_thresholded[i])\n",
    "        iou_value = iou(batch_y[i], y_pred_thresholded[i])\n",
    "        dice_values.append(dice)\n",
    "        iou_values.append(iou_value)\n",
    "    if len(loss_values) >= len(test_generator):\n",
    "        break\n",
    "\n",
    "average_dice = np.mean(dice_values)\n",
    "average_iou = np.mean(iou_values)\n",
    "\n",
    "print(\"Average test dice: \", average_dice)\n",
    "print(\"Average test IoU: \", average_iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb2330a-b4cb-4605-9d43-5d286235b581",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_case = 19\n",
    "\n",
    "dwi_path = os.path.join(base_path, 'rawdata', 'sub-strokecase{}'.format(\"%04d\" %example_case), 'ses-0001', 'dwi/', 'sub-strokecase{}_ses-0001_dwi.nii.gz'.format(\"%04d\" % example_case))\n",
    "mask_path = os.path.join(base_path, 'derivatives', 'sub-strokecase{}'.format(\"%04d\" %example_case), 'ses-0001', 'sub-strokecase{}_ses-0001_msk.nii.gz'.format(\"%04d\" % example_case))\n",
    "\n",
    "dwi_image = nib.load(dwi_path).get_fdata()\n",
    "mask_image = nib.load(mask_path).get_fdata()\n",
    "\n",
    "img_resize = lambda img, dims: cv2.resize(img[:,:], dims)\n",
    "\n",
    "dwi_image=img_resize(dwi_image, (112, 112))\n",
    "mask_image=img_resize(mask_image, (112, 112))\n",
    "print(\"dwi_image.shape: \", dwi_image.shape)\n",
    "print(\"mask_image.shape: \", mask_image.shape)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "slice2show=31\n",
    "ax1.imshow(dwi_image[:,:,slice2show], cmap='gray')\n",
    "ax1.set_title('Dwi')\n",
    "ax1.set_axis_off()\n",
    "\n",
    "\n",
    "ax2.imshow(mask_image[:,:,slice2show], cmap='gray')\n",
    "ax2.set_title('GT')\n",
    "ax2.set_axis_off()\n",
    "\n",
    "dwi_image=scaler.fit_transform(dwi_image.reshape(-1, dwi_image.shape[-1])).reshape(dwi_image.shape)\n",
    "\n",
    "X = np.zeros((72,112,112,1))\n",
    "for j in range(72):\n",
    "    X[j,:,:,0] =dwi_image[:,:,j]\n",
    "print(\"X.shape: \", X.shape)\n",
    "\n",
    "pred_wt=model.predict(X)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(3,3))\n",
    "ax.imshow(pred_wt[31,:,:,:],cmap='gray')\n",
    "\n",
    "y_pred_thresholded = pred_wt > 0.1\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(3,3))\n",
    "ax.imshow(y_pred_thresholded[31,:,:,:],cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84f6d04-fd8a-43c1-9210-7f870147e18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_score(y_true, y_pred, smooth = 1):\n",
    "    intersection = np.sum(y_true * y_pred)\n",
    "    total = np.sum(y_true) + np.sum(y_pred)\n",
    "    dice = (2 * intersection + smooth) / (total + smooth)\n",
    "    dice = round(dice, 3)\n",
    "    return dice\n",
    "\n",
    "def iou(y_true,y_pred, smooth = 1):\n",
    "    intersec = np.sum(y_true * y_pred)\n",
    "    union = np.sum(y_true + y_pred)\n",
    "    iou = (intersec + smooth) / (union- intersec + smooth)\n",
    "    iou = round(iou, 3)\n",
    "    return iou\n",
    "\n",
    "for i in range(5,60):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 4, 1)\n",
    "    plt.imshow(dwi_image[:,:,i], cmap='gray')\n",
    "    plt.title('Input')\n",
    "    plt.subplot(1, 4, 2)\n",
    "    plt.imshow(mask_image[:,:,i], cmap='gray')\n",
    "    plt.title('Ground Truth')\n",
    "    plt.subplot(1, 4, 3)\n",
    "    plt.imshow(pred_wt[i,:,:,:], cmap='gray')\n",
    "    plt.title('Predicted')\n",
    "    plt.subplot(1, 4, 4)\n",
    "    plt.imshow(y_pred_thresholded[i,:,:,:], cmap='gray')\n",
    "    plt.title('Threshold')\n",
    "    dice = dice_score(mask_image[:,:,i], y_pred_thresholded[i,:,:,:])\n",
    "    Iou = iou(mask_image[:,:,i], y_pred_thresholded[i,:,:,:])\n",
    "    plt.suptitle(f\"Sample_19_Slice_00{i}  ,Dice Score:{dice}  ,IOU:{Iou}\")\n",
    "    output_filename = f'Sample_19_Slice_00{i}.png'\n",
    "    output_path = os.path.join(OUTPUT_DIRECTORY, output_filename)\n",
    "    plt.savefig(output_path)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
